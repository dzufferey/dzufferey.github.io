# Bulk Synchronous Model

Up until now, we saw models where processes synchronize on some actions (lock, messages) and are independent on the others.
This model comes from the classical von Neumann architecture or SISD (single instruction, single data).
However, this is not an efficient architecture to process large datasets.
When the same program is executed over and over again on different data, a SIMD (single instruction, multiple data) architecture is more efficient.
In this model, the hardware itself takes care of the executing the program efficiently with a high degree of parallelism.
GPUs are (mostly) implementing SIMD architectures.


## Parameterized programs and general impossibility result

Before, we go deeper into specific model, let us look at what happens even for simple _parameterized_ models.
Here we will look at an important result presented in [Limits for Automatic Verification of Finite-state Concurrent Systems](https://www.cs.cornell.edu/~kozen/Papers/LimitsForAutomatic.pdf) by Krzysztof R. Apt and Dexter C. Kozen, 1986.

The first question is what is a simple model for programs in that setting.

### Parameterized programs

If we want to execute programs in parallel and take advantage of parallelism, it is important to know how many program there are.
For instance, if there are $D$ pieces of data and $N$ processes, we chunk the data into blocks of size $D/N$ and each process get one block.
Therefore, we can assume programs know what is the value of $N$ and each program has its own ID as a number in $[0;N)$.

A first, very general model is to represent a parameterize program as a function $P(N,i)$ such that $P(N,i)$ is a finite-state machine for every $N$ and $0â‰¤i<N$.
We can then represent our parallel programs as $âˆ_{i âˆˆ [0,N)} P(N,i)$ where $âˆ$ represent the (indexed) parallel composition.

### Parameterized verification

Given some property $Ï†(N)$ which evaluate over the states of $âˆ_{i âˆˆ [0,N)} P(N,i)$, the parameterized verification problem asks if the program satisfy the property no matter what $N$ is.

More formally, it is written as $âˆ€ N. âˆ_{i âˆˆ [0,N)} P(N,i) âŠ¢ Ï†(N)$ where $âŠ¢$ is entailment.
It means that $Ï†(N)$ evaluates to _true_ over the structure induced by $âˆ_i P(N,i)$.


### Impossibility result

Let us look first a specific values of $N$.
For a given value of $N$, e.g., $N=42$, we can use the product construction to deal with $âˆ$ and then use standard automata theoretic result to check the property (see [week 1](viewer.html?md=concurrency_theory_2018/notes_1.md)).

However, that question becomes undecidable for all $N$.
Surprisingly, the undecidablility result does not even use the parallel composition.

Let $M$ be a Turing machine.
We construct the following parameterized program $P(N,i)$
```
flag := false;
for k := 1 to N {
    simulate one step of M
}
if M has halted {
    flag := true
}
```
The final value of the `flag` is `true` iff $M$ halts within $N$ steps.

This program fits our model as for each $N$ it is finite state, i.e., it needs to simulate at most $N$ cells on the Turing machine's tape.
Notice that $P(N,i)$ does not even use $i$.

As property $Ï†$, we can ask if $flag$ ever becomes $true$ which is equivalent to deciding if $M$ halts.

_Remark._
This reduction is quite inefficient.
It is possible to have smaller reduction where each $P(N,i)$ simulates the $i$th cell of $M$'s tape.


### Parameterized programs vs Petri net

Earlier in the class, we saw encodings of concurrent programs into Petri nets.
In the earlier model, all the programs where the same independently on the number of threads ($N$) and they did not have ID.
Furthermore, they interacted over a finite memory.
These restrictions give rise to monotonic behaviors and we could encode such programs as Petri nets.

### Consequences

Since the simple model of parameterized programs is already Turing complete, we may want to consider:
- even more restricted models
- weaker or specialized properties
- using abstractions (sometimes adding more behaviors can make things easier)
- all of the above ğŸ˜€


## Models for SIMD programs

Models for SIMD programs fall under the umbrella of bulk synchrony.
In this class, we will discuss a simplified model based on the paper [The Design and Implementation of a Verification Technique for GPU Kernels](https://www.doc.ic.ac.uk/~afd/homepages/papers/pdfs/2015/TOPLAS.pdf) by Adam Betts, Nathan Chong, Alastair F. Donaldson, Jeroen Ketema, Shaz Qadeer, Paul Thomson, and John Wickerson, 2015.

### Syntax

We consider a model where all the programs $P(N,i)$ executes a single method composed of the following statements:

$\mathit{stmt}$ define the control flow:

$
\begin{array}{lcl}
\mathit{stmt} & ::= & \mathit{basicStmt} \\\\
              &   | & \mathit{stmt}; ~ \mathit{stmt} \\\\
              &   | & if ~ \mathit{localExpr} ~ \mathit{stmt} ~ else ~ \mathit{stmt} \\\\ 
              &   | & while ~ \mathit{localExpr} ~ \mathit{stmt} \\\\
              &   | & barrier
\end{array}
$


The $\mathit{basicStmt}$ are the assignments, read, and write operations:

$
\begin{array}{lcl}
\mathit{basicStmt} & ::= & \mathit{var} ~ := ~ \mathit{localExpr} \\\\
                   & |   & \mathit{var} ~ := ~ shared[\mathit{localExpr}] \\\\
                   & |   & shared[\mathit{localExpr}] ~ := ~ \mathit{localExpr}
\end{array}
$

Every thread has its own local copy of the variables ($\mathit{var}$) but the memory is shared.
We represent the memory as a single infinite array called $shared$.

Finally, the $\mathit{localExpr}$ are values and expressions over values.

$
\begin{array}{lcl}
\mathit{localExpr} & ::= & \mathit{var} \\\\
                   & |   & i \\\\
                   & |   & N \\\\
                   & |   & \mathit{constant} \\\\
                   & |   & \mathit{localExpr} ~ \mathit{op} ~ \mathit{localExpr} \\\\
\mathit{op}        & ::= & + \\\\
                   & |   & - \\\\
                   & |   & * 
\end{array}
$

where $i$ is the local thread identifier, $\mathit{var}$ is a variable identifier, $constant$ are numerical values, and $N$ is the number of threads.


### Semantics (synchronous, delayed visibility)

Before giving the details of the semantics, let us explain some specificities of the model.

#### Lockstep (predicated) executions
    
In a SIMD program, all threads execute together in lockstep.
This means that they follow exactly the same path.
When encountering a branch or loop, the program must execute every path at the same time.
This is achieved through _predicated_ execution.

The hardware has special registers which carries the values of predicate (expression) and the statements only have an effect when the predicate is true.
For instance, in the case of branches, both side are executed and the predicate's value decide which path has its result committed.

For instance, consider the program:
```
x := 0;
y := 0;
if (i < m) {
    x := 1;
} else {
    y := 1;
}
```
What get executed is closer to
```
x := 0;
y := 0;
p := i < m;
p â‡’ x := 1;
Â¬p â‡’ y := 1;
```
The notation $p â‡’ \mathit{stmt}$ means the effect the statements (writing to a variable/memory) only happens when $p$ is true.

For loops
```
x := i;
while (x > 0) {
    x := x - 1;
}
```
we get something similar:
```
x := i;
p := x > 0;
while (âˆƒ i. i.p) {
    p â‡’ x := x - 1;
    p â‡’ p := x > 0;
}
```
As long as one thread is in the loop, evey thread stays in the loop.

Downside is that for each branch, both path are executed (lot of operations are thrown away) and loop is executed as long as one thread is in the loop.
Therefore, GPUs are more efficient on code which is very regular and has a simple control flow, e.g., matrix multiplication.

__Remark.__
Predicated execution is very similar to the mechanism used for speculative execution.


#### Barrier and data races

Memory accesses also have some limitations.
In particular, a thread should not write or read from a location written by another thread unless a $barrier$ has been executed.
$barrier$ synchronizes the state of the memory accross all threads but between barrier the thread memory accesses must not overlap.

The following program is not correct:
```
if (i > 0) {
    shared[i] := (shared[i-1] + shared[i]) / 2;
}
```
But it should be written as
```
if (i > 0) {
    tmp1 := shared[i-1];
}
barrier;
if (i > 0) {
    tmp2 := shared[i];
    shared[i] := (tmp1 + tmp2) / 2;
}
```

#### Rules

We decompose the transition rules into local rules for the execution of the statements and global rules for the lockstep semantics of all the threads.

We will use $t$ to denote the state a thread.
* $t.l$ means accessing $l$ at thread $t$.
* $t.l â† e$ returns a new state where $l$ has value $e$ and the rest is unchanged.
* $R$, $W$ are special variables to represent the read and written memory locations. They do not occur in the origial program.
* $i$, $N$ can be access but not written.

__Shadow memory.__
To simplify the semantics, each thread keeps a local copy of the global memory.
These copies are synchronized when a barrier occurs.


__Local rules.__

We write $eâ†“t$ for the evaluation of a $\mathit{localExpr}$ $e$ at thread $t$.

Local rules have a thread state and predicated $\mathit{basicStmt}$ on the left hand side and return a new thread state.

* skip
  \\[{
  Â¬t.p
  }\over{
  (t, (p â‡’ \mathit{basicStmt})) â†’ t
  }\\]
* assignment
  \\[{
  t.p \qquad  t' = (t.l â† eâ†“t)
  }\over{
  (t, (p â‡’ l := e)) â†’ t'
  }\\]
* read
  \\[{
  t.p \qquad  v = eâ†“t \qquad t' = (t.l â† t.shared[v]) \qquad tâ€³ = (t'.R â† t'.R âˆª v)
  }\over{
  (t, (p â‡’ l := shared[e])) â†’ tâ€³
  }\\]
* write
  \\[{
  t.p \qquad vâ‚ = eâ‚â†“t \qquad vâ‚‚ = eâ‚‚â†“t \qquad t' = (t.shared[vâ‚] â† vâ‚‚) \qquad tâ€³ = (t'.W â† t'.W âˆª vâ‚)
  }\over{
  (t, (p â‡’ shared[eâ‚] := eâ‚‚)) â†’ tâ€³
  }\\]

__Synchronization rules.__

Let $T = (tâ‚, ~â€¦, ~ t_N)$ be the state of all threads and $T[j]$ the $j$th thread in $T$ ($T[j].i = j$).

To check for data race we use the following predicate:

$
race(T) ~~ â‡” ~~ âˆƒ k,l.~ kâ‰ l âˆ§ T[k].W âˆ© (T[l].R âˆª T[l].W) â‰  âˆ…
$

And a function to merge the copies of the shared memory:

$
merge(T)[l] = \\left\\{\begin{array}{ll}
  T[j].shared[l] \qquad &   \text{if} ~ âˆƒ j.~ l âˆˆ T[j].W \\\\
  T[0].shared[l]        &   \text{otherwise}
\end{array}\\right.
$


* basic statements
  \\[{
  âˆ€ j. ~ (T[j], p â‡’ basicStmt) â†’ T'[j]  \qquad  Â¬race(T')
  }\over{
  (T, p â‡’ basicStmt) â†’ T'
  }\\]
* date race
  \\[{
  âˆ€ j. ~ (T[j], pâ‡’ basicStmt) â†’ T'[j]  \qquad  race(T')
  }\over{
  (T, p â‡’ basicStmt) â†’ error
  }\\]
* barrier skip
  \\[{
  âˆ€ j. ~ Â¬T[j].p
  }\over{
  (T, p â‡’ barrier) â†’ T
  }\\]
* barrier error
  \\[{
  âˆƒ j, k. ~ T[j].p âˆ§ Â¬T[k].p
  }\over{
  (T, p â‡’ barrier) â†’ error
  }\\]
* barrier sync
  \\[{
  âˆ€ j. ~ T[j].p \qquad T'[j] = ((T[j].shared â† merge(T)).R â† âˆ…).W â† âˆ…
  }\over{
  (T, p â‡’ barrier) â†’ T'
  }\\]

__Control-flow rules.__

Finally, we need a few rules for the control flow.
These rules work on sequences of statements.

We write "$a :: b$" to extract the head of a sequence of statement or extend a sequence.
$::$ is not the same as $;$.
$::$ is for predicated statement and $;$ for statements.
$â‡’$ binds more strongly than $::$, i.e.,  "$p â‡’ s :: s$" is "$(p â‡’ s) :: s$".

* basic
  \\[{
  (T, p â‡’ \mathit{basicStmt} :: s) â†’ T'
  }\over{
  (T, p â‡’ \mathit{basicStmt} :: s) â†’ (T', s)
  }\\]
* barrier
  \\[{
  (T, p â‡’ barrier :: s) â†’ T'
  }\over{
  (T, p â‡’ barrier :: s) â†’ (T', s)
  }\\]
* sequence
  \\[{}\over{
  (T, p â‡’ sâ‚; sâ‚‚ :: s) â†’ (T, pâ‡’ sâ‚ :: pâ‡’ sâ‚‚ :: s)
  }\\]
* if
  \\[{
  \text{fresh} ~ pâ‚, pâ‚‚
  }\over{
  (T, p â‡’ if ~ c ~ sâ‚ ~ else ~ sâ‚‚ ~::~ s) â†’ (T, pâ‚ := p âˆ§ c ~::~ pâ‚‚ := p âˆ§ Â¬c ~::~ pâ‚ â‡’ sâ‚ ~::~ pâ‚‚ â‡’ sâ‚‚ ~::~ s)
  }\\]
* while
  \\[{
  \text{fresh} ~ pâ‚ \qquad âˆƒ j. T[j].p
  }\over{
  (T, p â‡’ while ~ c ~ b ~::~ s) â†’ (T, pâ‚ := p âˆ§ c ~::~ pâ‚ â‡’ b ~::~ pâ‚ â‡’ while ~ c ~ b ~::~ s)
  }\\]
  \\[{
  âˆ€ j. ~ Â¬T[j].p
  }\over{
  (T, p â‡’ while ~ c ~ b ~::~ s) â†’ (T, s)
  }\\]
* error
  \\[{
  (T, p â‡’ basicStmt) â†’ error
  }\over{
  (T, p â‡’ basicStmt :: s) â†’ error
  }\\]


### Pairwise Properties 

Instead of looking at arbitrary properties, we are going to limit ourselves to check the absence of data race and incorrect barrier.
These are the two properties that cause errors at the level of the semantics.

What is important to notice is that these properties are _pairwise_ properties.
To violate these properties we need only two processes:
- data race: $âˆƒ k,l. ~ kâ‰ l âˆ§ T[k].W âˆ© (T[l].R âˆª T[l].W) â‰  âˆ…$
- barrier error: $âˆƒ j, k.~ T[j].p âˆ§ Â¬T[k].p$

This will be the crux of the two threads reduction.

### Adversarial abstraction

Until now, nothing prevents the undecidability argument to be adapted to this model.
To simplify the problem, we need to apply some abstraction.
The paper proposes an adversarial abstraction.

The shared state is ignored.
Read operations are replaced nondeterministic assignments.
The intuition is that the properties we look at most often depends on the control-flow and the local variables rather than the data.

The abstraction replace the read and write rules with:
* read
  \\[{
  t.p \qquad  v = eâ†“t \qquad  t' = (t.l â† v').R â† t.R âˆª v
  }\over{
  (t, (p â‡’ l := shared[e])) â†’ t'
  }\\]
* write
  \\[{
  t.p \qquad  v = eâ‚â†“t \qquad  t' = (t.shared â† shared').W â† t.W âˆª v
  }\over{
  (t, (p â‡’ shared[eâ‚] := eâ‚‚)) â†’ t'
  }\\]

With an adversarial abstraction, the problem become simpler as it is not possible anymore to encode a Turing machine.
The abstraction still contains the traces of the original program.
Therefore, if the abstracted program is correct then the original program is also correct.
However, it is possible that a correct program becomes incorrect with the adversarial abstraction.


[The paper](https://www.doc.ic.ac.uk/~afd/homepages/papers/pdfs/2015/TOPLAS.pdf) also describe a more advanced "equality abstraction".


### Two threads reduction (cut-off bound)

Even though we want to check that a program is correct for any `N`.
We can look at only two threads.
This property that all the bug show up for a finite `N` is called a cut-off bound.
In this case, it is `2`.

Applying the adversarial abstraction _decouples_ the threads.
The read and write of each thread is independent from all the other thread.
Since the properties we are looking at are pairwise we just need to find the two threads which trigger the error.
However, we don't know which one a priori.

The idea is to create a single program which nondeterministically guesses which two threads are causing the error and then executes these two thread in lockstep.
Then, it is possible to use an off-the-shelf [symbolic execution engine](https://en.wikipedia.org/wiki/Symbolic_execution) to check the programs are correct.

Let look at an example inspired from [this code](https://github.com/KhronosGroup/OpenCL-CTS/blob/cl22_trunk/test_conformance/basic/test_barrier.c).
I am taking a bit of freedom with the notation...

```c
sum(global int[] a, int size, global int[] tmp_sum, global int* sum)
{
    tmp_sum[i] := 0;
    for (int k:=i; k < size; k+=N) {
        tmp_sum[i] += a[k];
    }

    //each iteration fold the partial sum in half
    local int n := N; //number of element to process
    for (int k := (n+1)/2; n>1; k := (k+1)/2)
    {
        barrier();
        if (i + k < n)
            tmp_sum[i] += tmp_sum[i + k];
        n := k;
    }

    if (i = 0)
        *sum := tmp_sum[0];
}
```

The first part is to partially instrument for our semantics (read/write, loops):
```c
sum(global int[] a, int size, global int[] tmp_sum, global int* sum)
{
    R := âˆ…;
    W := âˆ…;
    tmp_sum[i] := 0;
    W := W âˆª {tmp_sum[i]};
    local int k := i;
    C := k < size;
    while(C){
        tmp_sum[i] += a[k];
        R := R âˆª {a[k], tmp_sum[i]};
        W := W âˆª {tmp_sum[i]};
        k += N;
        C := k < size;
    }

    local int n := N;
    k := (n+1)/2
    C = n > 1;
    while (C) {
        barrier();
        R := âˆ…;
        W := âˆ…;
        if (i + k < n) {
            tmp_sum[i] += tmp_sum[i + k];
            R := R âˆª {tmp_sum[i], tmp_sum[i+k]};
            W := W âˆª {tmp_sum[i]};
        }
        n := k;
        k := (k+1)/2;
        C := n > 1;
    }

    if (i = 0) {
        *sum := tmp_sum[0];
        R := R âˆª {tmp_sum[0]};
        W := W âˆª {sum};
    }
}
```

Then we can apply the adversarial abstraction (remove read and write, predicated statements).
To simulate the predicated execution, we use the ternary conditional operator `condition ? true_val : false_val`.
```c
sum(global int[] a, int size, global int[] tmp_sum, global int* sum)
{
    R := âˆ…;
    W := âˆ…;
    W := W âˆª {tmp_sum[i]};
    k := i;
    C := k < size;
    while(C){
        R := C ? R âˆª {a[k], tmp_sum[i]} : R;
        W := C ? W âˆª {tmp_sum[i]} : W;
        k := C ? k+N : k;
        C := C ? k < size : C;
    }

    n := N;
    k := (n+1)/2
    C := n > 1;
    while (C) {
        barrier();
        R := C ? âˆ… : R;
        W := C ? âˆ… : W;
        R := C && (i + k < n) ? R : R âˆª {tmp_sum[i], tmp_sum[i+k]};
        W := C && (i + k < n) ? W : W âˆª {tmp_sum[i]};
        n := C ? k : n;
        k := C ? (k+1)/2 : k;
        C := C ? n > 1 : C;
    }

    R := (i = 0) ? R : R âˆª {tmp_sum[0]};
    W := (i = 0) ? W âˆª {sum};
}
```

Finally, we can do the two threads encoding and finish the instrumentation:
```c
sum(global int[] a, int size, global int[] tmp_sum, global int* sum)
{
    assume(i â‰  j);
    Râ‚ := âˆ…;                            Râ‚‚ := âˆ…;
    Wâ‚ := âˆ…;                            Wâ‚‚ := âˆ…;
    Wâ‚ := Wâ‚ âˆª {tmp_sum[i]};            Wâ‚‚ := Wâ‚‚ âˆª {tmp_sum[j]};
    assert(tmp_sum[i] âˆ‰ Wâ‚‚);            assert(tmp_sum[j] âˆ‰ Wâ‚);
    int kâ‚ := i;                        int kâ‚‚ := j;
    Câ‚ := kâ‚ < size;                    Câ‚‚ := kâ‚‚ < size;
    while(Câ‚ || Câ‚‚) {
        Râ‚ := Câ‚ ? Râ‚ âˆª {a[kâ‚], tmp_sum[i]} : Râ‚;   Râ‚‚ := Câ‚‚ ? Râ‚‚ âˆª {a[kâ‚‚], tmp_sum[j]} : Râ‚‚;
        Wâ‚ := Câ‚ ? Wâ‚ âˆª {tmp_sum[i]} : Wâ‚;          Wâ‚‚ := Câ‚‚ ? Wâ‚‚ âˆª {tmp_sum[j]} : Wâ‚‚;
        assert(Â¬Câ‚ âˆ¨ tmp_sum[i] âˆ‰ Wâ‚‚);              assert(Â¬Câ‚‚ âˆ¨ tmp_sum[j] âˆ‰ Wâ‚);
        assert(Â¬Câ‚ âˆ¨ a[kâ‚] âˆ‰ Wâ‚‚);                   assert(Â¬Câ‚‚ âˆ¨ a[kâ‚‚] âˆ‰ Wâ‚);
        kâ‚ := Câ‚ ? kâ‚+N : kâ‚;                       kâ‚‚ := Câ‚‚ ? kâ‚‚+N : N;
        Câ‚ := Câ‚ ? kâ‚ < size : Câ‚;                  Câ‚‚ := Câ‚‚ ? kâ‚‚ < size : Câ‚‚;
    }


    nâ‚ := N;                            nâ‚‚ := N;
    kâ‚ := (nâ‚+1)/2                      kâ‚‚ := (nâ‚‚+1)/2
    Câ‚ := nâ‚ > 1;                       Câ‚‚ := nâ‚‚ > 1;
    while (Câ‚ || Câ‚‚) {
        assert( Câ‚ = Câ‚‚ );
        Râ‚ := Câ‚ ? âˆ… : Râ‚;                                                  Râ‚‚ := Câ‚‚ ? âˆ… : Râ‚‚;
        Wâ‚ := Câ‚ ? âˆ… : Wâ‚;                                                  Wâ‚‚ := Câ‚‚ ? âˆ… : Wâ‚‚;
        Râ‚ := Câ‚ && (i + kâ‚ < nâ‚) ? Râ‚ âˆª {tmp_sum[i], tmp_sum[i+kâ‚]} : Râ‚;  Râ‚‚ := Câ‚‚ && (i + kâ‚‚ < nâ‚‚) ? Râ‚‚ âˆª {tmp_sum[j], tmp_sum[j+kâ‚‚]} : Râ‚‚;
        Wâ‚ := Câ‚ && (i + kâ‚ < nâ‚) ? Wâ‚ âˆª {tmp_sum[i]} : Wâ‚;                 Wâ‚‚ := Câ‚‚ && (i + kâ‚‚ < nâ‚‚) ? Wâ‚‚ âˆª {tmp_sum[j]} : Wâ‚‚;
        assert(Â¬Câ‚ âˆ¨ tmp_sum[i] âˆ‰ Wâ‚‚);                                      assert(Â¬Câ‚‚ âˆ¨ tmp_sum[j] âˆ‰ Wâ‚)
        assert(Â¬Câ‚ âˆ¨ tmp_sum[i+kâ‚] âˆ‰ Wâ‚‚);                                   assert(Â¬Câ‚‚ âˆ¨ tmp_sum[j+kâ‚‚] âˆ‰ Wâ‚);
        nâ‚ := Câ‚ ? kâ‚ : nâ‚;                                                 nâ‚‚ := Câ‚‚ ? kâ‚‚ : nâ‚‚;
        kâ‚ := Câ‚ ? (kâ‚+1)/2 : kâ‚;                                           kâ‚‚ := Câ‚‚ ? (kâ‚‚+1)/2 : kâ‚‚;
        Câ‚ := Câ‚ ? nâ‚ > 1 : Câ‚;                                             Câ‚‚ := Câ‚‚ ? nâ‚‚ > 1 : Câ‚‚;
    }

    Râ‚ := (i = 0) ? Râ‚ : Râ‚ âˆª {tmp_sum[0]};     Râ‚‚ := (j = 0) ? Râ‚‚ : Râ‚‚ âˆª {tmp_sum[0]};
    Wâ‚ := (i = 0) ? Wâ‚ âˆª {sum};                 Wâ‚‚ := (j = 0) ? Wâ‚‚ âˆª {sum};
    assert(i â‰  0  âˆ¨  sum âˆ‰ Wâ‚‚);                 assert(j â‰  0  âˆ¨  sum âˆ‰ Wâ‚);
}
```
